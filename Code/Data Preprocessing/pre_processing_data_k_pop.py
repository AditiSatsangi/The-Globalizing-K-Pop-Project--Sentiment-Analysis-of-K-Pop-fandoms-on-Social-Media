# -*- coding: utf-8 -*-
"""Pre-Processing Data- K Pop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QzXUDGwNHdqBfs7UoVT3jyr_VwWn9qjO

## Importing Libraries
"""

!pip install stop-words
!pip install pyLDAvis
!pip install langdetect
!pip install googletrans
!pip install googletrans==4.0.0-rc1
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('averaged_perceptron_tagger')

# Download NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('popular')
nltk.download('omw-1.4')

!pip install pprintpp
import pprintpp as pprint
import warnings

warnings.filterwarnings('ignore', category=DeprecationWarning)

!pip install keybert

"""## Data Collection"""

import numpy as np
import pandas as pd

df = pd.read_excel('/content/SF9_fanclub_conv_followers_100_500.xlsx')

df.head()

df.shape

# Create a new column with author_id and text combined
df['author_text'] =  df['author_id_x'].astype(str) + '- ' + df['text']

# Now perform the aggregation
conversation_df = df.groupby('conversation_id').agg({
    'author_text': '...'.join,  # Concatenate author_id and text
    'id': 'count',
    'first_tweet': 'first'
}).reset_index()

# If you want the result in the `text` column
conversation_df.rename(columns={'author_text': 'text'}, inplace=True)

conversation_df.head()

conversation_df['text'][0]

df.shape

conversation_df.shape

# Rename columns for clarity
conversation_df.columns = ['conversation_id', 'text', 'reply_count','first_tweet']

# Display the DataFrame
conversation_df.head()

# Rename columns for clarity
conversation_df.shape

conversation_df['text']= conversation_df['first_tweet'] + '...' + conversation_df['text']

conversation_df.head()

conversation_df['text'][0]

conversation_df['first_tweet'][0]

conversation_df['reply_count'][0]

conversation_df['text'][9]

conversation_df.to_csv('conversation_df.csv', index=False)

conversation_df.head()

"""## Data Pre-Processing

### Removing Emojis
"""

from langdetect import detect
from googletrans import Translator, LANGUAGES

def detect_and_translate(text):
    # Initialize the translator
    translator = Translator()

    try:
        # Detect the language
        detected_lang = detect(text)
        print(f"Detected language: {LANGUAGES.get(detected_lang, 'Unknown')}")

        # Check if the detected language is English
        if detected_lang != 'en':
            # Translate to English
            translated = translator.translate(text, src=detected_lang, dest='en')
            return translated.text
        else:
            return text
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

import re
from urllib.parse import urlparse

def remove_emoji(text):
    if isinstance(text, str):  # Check if text is a string
        emoji_pattern = re.compile(
            "["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002702-\U000027B0"  # additional symbols
            u"\U0001F900-\U0001F9FF"  # supplemental symbols and pictographs
            u"\U0001FA70-\U0001FAFF"  # additional symbols and pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F700-\U0001F77F"  # alchemical symbols
            u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
            u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
            u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            u"\U0001FA00-\U0001FA6F"  # Chess Symbols
            u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)
    else:
        return text  # Return the original value if it's not a string

def replace_urls_with_link(text):
    if isinstance(text, str):  # Check if text is a string before applying regex
        return re.sub(r'https?://\S+', 'link', text)
    else:
        return text  # Return the original value if it's not a string

def remove_special_characters(text):
    if isinstance(text, str):  # Check if text is a string before applying regex
        return re.sub(r'[\\@#$%,]', '', text)
    else:
        return text  # Return the original value if it's not a string

def clean_text(text):
    text = remove_emoji(text)
    text = replace_urls_with_link(text)
    text = remove_special_characters(text)
    text = detect_and_translate(text)
    if isinstance(text, str):  # Check if text is a string before stripping whitespace
        text = re.sub(r'\s+', ' ', text).strip()
    return text

conversation_df['Text'] = conversation_df['text'].apply(clean_text)

conversation_df.head()

"""### Removing Words"""

words_to_remove = [ 'keonhee' ,'kebinwooo', 'mindofstacey','dprian_','comicsanas','dahlihwa', 'redcherrydream','haonslut','kpop'
                   'castle_tak', 'jellywangji', 'meg_marvels','sonderstarlight','ldwww_w_',
                    'dimiiuc','jaengpup', 'twowomenkpop','pumseongs', 'teume',  "euijeongify",
                    "johnny" ,"kenneth_iain",'jacquelame', 'taesanade',  'enbyuno','bora' , 'aishaishious',
                    'midsummer_jy', '_fix_ation', 'gosiauxkimmy','loveisland','hyunlixxi',
                    'starrrcha', 'peachyahxo', 'fandom', 'xion_lemons', '_padii_' 'si', 'lin',
    'garam', 'froggienoya','naziatheworld', 'instinctangel', 'kxcvxvii','jungkook','morganbbang', 'chu', 've',
        'spcygomtokki', 'em', 'guy', '.', 'le', 'blackpink', 'pink', 'jisoo', 'jin', 'Le Sserafim',
    'sserafim', 'kazuha', 'sakura', 'sserafim', 'jake', 'yoongi', 'eunchae', 'chaewon', 'yunjin', 'enhypen', 'bts',
    'selenators', 'em', 'gon','lf moot', 'sm', 'im', 'beckysangels.', 'taylorswift13', 'maxeddedd', 'soompi', 'allkpop', 'lisa',
    'guy', 'amp', 'taehyung', 'liediminie', 'taylorshift13', 'eunchae', 'jennie', 'le sserafim', 'enhypen', 'lesserafim',
    'imlesserafim', 'sserafim', 'kim', 'youngbin', 'babegnf','ot7', 'bangton','seokjin', 'gdragon', 'gee', 'inseong', 'jaeyoon', 'dawon', 'zuho',
    'yoo taeyang', 'inc', 'em', 've', 'wm', 'hwiyoung', 'chani.', 'rowoon', 'sf9', 'binnie', 'haechan', 'baby', 'twibbon',
    'don', 'bts_bighit', 'bad','bts', 'bts_twt', 'hobi', 'jimin', 'tannies', 'cjnismygfanot7', '_gal_ot7', 'exo', 'nct',
    'ateez', 'stray kids', 'seventeen', 'twice', 'red velvet', 'itzy', 'mamamoo', 'super junior', 'shinee', 'got7',
    'bigbang', 'monsta x', 'gfriend', 'izone', 'i.o.i', 'loona', 'everglow', 'fromis_9', 'gugudan', 'wjsn', 'dreamcatcher',
    'astro', 'victon', 'sf9', 'pentagon', 'cix', 'cravity', 'verivery', 'onf', 'golden child', 'oneus', 'wei',
    'the boyz', 'ateez', 'ab6ix', 'xdinary heroes', 'kepler', 'ive', 'purple kiss', 'newjeans', 'dkz', 'tempest',
    'yena', 'chaeyeon', 'hyunjin', 'felix', 'bangchan', 'lee know', 'changbin', 'han', 'seungmin', 'i.n', 'woozi',
    'hoshi', 'mingyu', 'jeonghan', 'joshua', 'jun', 'dino', 'the8', 'vernon', 'wonwoo', 'dk', 'seungkwan', 's.coups',
    'jinyoung', 'mark', 'jackson', 'jb', 'youngjae', 'bamBam', 'yugyeom','byk', 'skrg', 'carat' , 'yooyawnzzn'
]

from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.wordnet import WordNetLemmatizer
from gensim import corpora, models
import pandas as pd
import gensim
import pyLDAvis.gensim
import pprintpp as pprint
import warnings

warnings.filterwarnings('ignore', category=DeprecationWarning)

pattern = r'\b[^\d\W]+\b'
tokenizer = RegexpTokenizer(pattern)
en_stop = get_stop_words('en')
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Tokenize the text using spaCy
    # clean and tokenize document string
    raw = str(text).lower()
    tokens = tokenizer.tokenize(raw)


    # Remove stop words from tokens
    stopped_tokens = [token for token in tokens if token not in en_stop]

    # Lemmatize tokens
    lemma_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens]

    # Remove words containing only single characters
    new_lemma_tokens = [token for token in lemma_tokens if len(token) > 1 ]

    return ' '.join(new_lemma_tokens)



# Apply the function to the DataFrame
conversation_df['Text'] = conversation_df['Text'].apply(preprocess_text)

"""### Keywords Filtering"""

all_keywords= ['safe place','safe space', 'feeling sick','understanding','humanity','cries','appreciate','motivate','miserable','how are you',
               'working hard','doing okay','tired','sleep','pain','pissed','pray','panicked','good vibes',
               'understand','praying','positive energy','please help','help','missing','sending hugs','lost',
                "hurts", "grief", "mwah", "something wrong", "confused", "what should I do",
                'yonghee'	, 'midsummer_jy', 'loveisland', 'gosiauxkimmy', 'hyunlixxi',
    "what can I do", "how do I", "how to", "personal", "care", "true friend",
    "true friends", "hide", "caring", "feel better", "sending you hugs", "ily",
    "anyone know", "doesn't help", "don't worry", "thankful", "be well", "hard work",
    "better", "rough", "kind", "kindness", "miss", "hit me hard", "struggling",
    "struggle", "challenging", "challenge", "fighting hard", "tough", "overcoming",
    "lonely", "alone", "isolated", "missing", "understand", "relate to", "empathize",
    "support", "feeling", "feeling well", "emotional", "heartbroken", "heartbreak",
    "frustrated", "heart", "PTSD", "OCD", "panic attack", "panicking", "trauma",
    "loss", "failure", "I need someone to talk to", "I feel alone", "Does anyone understand",
    "relationship issues", "academic stress", "stressed", "anxious", "depressed",
    "help", "assistance", "support", "advice", "family issues", "financial issues",
    "financial problem", "broken", "sad", "problem", "esteem", "emotional", "bad day",
    "feeling blue", "mental", "mental health", "bad day", "depression", "anxiety",
    "issues", "exam", "exams", "empathy"
]

conversation_df = conversation_df[conversation_df['Text'].str.contains('|'.join(all_keywords), case=False)]

conversation_df.shape

conversation_df.head()

import re

def remove_words(text, words_to_remove):
    # Create a regex pattern to match any of the words to remove
    pattern = re.compile(r'\b(?:' + '|'.join(map(re.escape, words_to_remove)) + r')\b', re.IGNORECASE)
    # Substitute the matched words with an empty string
    cleaned_text = pattern.sub('', text)
    # Remove any extra whitespace left after removing words
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

conversation_df['Text'] = conversation_df['Text'].apply(lambda x: remove_words(x, words_to_remove))

conversation_df['cleaned_text']= conversation_df['Text'].copy()

documents= conversation_df['Text']
filtered_df= conversation_df.copy()

filtered_df['Text'] = filtered_df['cleaned_text'].apply(lambda x: x.split())

filtered_df.head()

filtered_df['text'][3]

filtered_df['reply_count'][3]

filtered_df.to_csv('Sf9-preprocessed_df_Dataset.csv', index=False)

filtered_df.shape

