# -*- coding: utf-8 -*-
"""Final SF9 without group names -K Pop..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i-hdc9CkKfKcLECtmv93rHjU0ug8_epW

## Importing Libraries
"""

!pip install stop-words
!pip install pyLDAvis
!pip install langdetect
!pip install googletrans
!pip install googletrans==4.0.0-rc1
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('averaged_perceptron_tagger')

# Download NLTK data files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('popular')
nltk.download('omw-1.4')

!pip install pprintpp
import pprintpp as pprint
import warnings

warnings.filterwarnings('ignore', category=DeprecationWarning)

!pip install keybert

"""## Data Collection"""

import numpy as np
import pandas as pd

conversation_df= pd.read_csv('Sf9-preprocessed_df_data.csv')

documents= conversation_df['Text']
filtered_df= conversation_df.copy()

filtered_df['Text'] = filtered_df['cleaned_text'].apply(lambda x: x.split())

filtered_df.head()

filtered_df.shape

filtered_df['Text'][0]

filtered_df['reply_count'][0]

filtered_df['first_tweet'][0]

filtered_df['text'][3]

filtered_df['reply_count'][3]

filtered_df.to_csv('Sf9-preprocessed_df_data.csv', index=False)

"""## POS Tagging"""

from nltk.corpus import stopwords
from nltk import pos_tag, word_tokenize

# Define extended POS tags to remove
pos_to_remove = {
    'IN',  # Prepositions
    'CC',  # Conjunctions
    'DT',  # Determiners
    'PRP', 'PRP$', 'WP', 'WP$',  # Pronouns
    'MD',  # Modal Verbs
    'CD'   # Numbers
}

filtered_df.head()

documents= filtered_df['Text']

# Function to filter tokens based on POS tags
def filter_tokens(doc, pos_to_remove):
    tokens = word_tokenize(doc)
    pos_tags = pos_tag(tokens)
    filtered_tokens = [word for word, pos in pos_tags if pos not in pos_to_remove]
    return filtered_tokens

# Apply the function to the 'Text' column
filtered_df['Text'] = filtered_df['cleaned_text'].apply(lambda doc: filter_tokens(doc, pos_to_remove))

# Join filtered tokens to create the filtered documents
filtered_df['cleaned_text'] = filtered_df['Text'].apply(lambda tokens: ' '.join(tokens))

from transformers import BertTokenizer, BertForSequenceClassification
import torch

"""# Topic Modelling"""

from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.corpora import Dictionary
from gensim.models.ldamodel import LdaModel
from gensim.models.coherencemodel import CoherenceModel
from sklearn.model_selection import train_test_split
import numpy as np

"""## LDA"""

# Create the TF-IDF representation
filtered_documents = filtered_df['cleaned_text'].tolist()
tfidf_vectorizer = TfidfVectorizer(max_features=500)
tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_documents)

# Extract the top 500 features and their corresponding indices
top_features = tfidf_vectorizer.get_feature_names_out()

# Filter the tokenized documents to keep only the top 500 features
# Ensure words are tokenized before filtering
filtered_docs = [[word for word in word_tokenize(doc) if word in top_features] for doc in filtered_documents]

# Create a dictionary and corpus from the filtered documents
dictionary = Dictionary(filtered_docs)
corpus = [dictionary.doc2bow(doc) for doc in filtered_docs]

# Get the dense representation of the TF-IDF matrix
tfidf_dense = tfidf_matrix.todense()

# Normalize each document vector to ensure the sum of elements equals 1
tfidf_dense = tfidf_dense / tfidf_dense.sum(axis=1)

# Convert the TF-IDF matrix to a format suitable for LDA
tfidf_corpus = [
    [(i, float(tfidf_dense[row, i])) for i in range(len(top_features)) if tfidf_dense[row, i] > 0]
    for row in range(tfidf_dense.shape[0])
]

# Split the TF-IDF corpus into train and test sets
train_tfidf_corpus, test_tfidf_corpus = train_test_split(tfidf_corpus, test_size=0.2, random_state=42)

# Set the number of topics
num_topics = 5

# Train the LDA model using the TF-IDF corpus
lda_model = LdaModel(corpus=train_tfidf_corpus, id2word=dictionary, num_topics=num_topics, random_state=42)

# Calculate coherence for the trained LDA model
coherence_model_lda = CoherenceModel(model=lda_model, texts=filtered_docs, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print(f'Coherence Score: {coherence_lda}')

# Print the top topics
for i, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):
    print(f"Topic {i}: {topic}")
    print('\n')

# Evaluate on the test set
test_perplexity = lda_model.log_perplexity(test_tfidf_corpus)
print(f'Test Perplexity: {test_perplexity}')

"""## pyLDAvis Visualization"""

import pyLDAvis.gensim_models

# Prepare the visualization
lda_vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)

# Display the visualization
pyLDAvis.display(lda_vis)

pyLDAvis.save_html(lda_vis, 'lda_pylDAvis.html')

import pprintpp as pprint
import warnings

warnings.filterwarnings('ignore', category=DeprecationWarning)

# Get topic distribution for each document
topic_distributions = lda_model.get_document_topics(corpus)

# Convert to DataFrame for easier analysis
topic_distributions_df = pd.DataFrame([{topic: prob for topic, prob in doc} for doc in topic_distributions])
print(topic_distributions_df)

def get_top_documents_for_topics(topic_distributions, num_docs):
    topic_docs = {}
    for doc_id, doc_topics in enumerate(topic_distributions):
        for topic_id, prob in doc_topics:
            if topic_id not in topic_docs:
                topic_docs[topic_id] = []
            topic_docs[topic_id].append((doc_id, prob))

    # Sort documents by probability and select top documents
    top_documents = {}
    for topic_id, docs in topic_docs.items():
        sorted_docs = sorted(docs, key=lambda x: -x[1])[:num_docs]
        top_documents[topic_id] = [doc_id for doc_id, _ in sorted_docs]

    return top_documents

# Get top documents for each topic
top_documents_per_topic = get_top_documents_for_topics(topic_distributions,10)

# Display the original texts corresponding to the top documents for each topic
for topic_id, doc_ids in top_documents_per_topic.items():
    print(f"Top documents for Topic {topic_id}:")
    for doc_id in doc_ids:
        # Use iloc to access the document based on its position
        print(f"  Document {doc_id}: {filtered_df['text'].iloc[doc_id]}")
        print()
    print()

# Get top documents for each topic
top_documents_per_topic = get_top_documents_for_topics(topic_distributions,5)
for topic_id, doc_ids in top_documents_per_topic.items():
    print(f"Top documents for Topic {topic_id}:")
    for doc_id in doc_ids:
        # Use iloc to access the document based on its position
        print(f"  Document {doc_id}: {filtered_df['text'].iloc[doc_id]}")
        print()
    print()

from sklearn.feature_extraction.text import TfidfVectorizer

documents = filtered_df['cleaned_text'].tolist()
# Fit the vectorizer to the documents and transform them into a TF-IDF document-term matrix
tfidf_doc_term_matrix = tfidf_vectorizer.fit_transform(documents)

# Get the topic_distribution for each document
doc_topic_matrix = []
for doc in corpus:
    doc_topic_matrix.append(lda_model.get_document_topics(doc))

doc_topic_matrix

'''
for topic_id, doc_identifiers in top_docs_per_topic.items():
    # Retrieve the actual documents using the identifiers
    top_docs_texts = [doc for doc in documents if doc in doc_identifiers]

    # Store these documents in the dictionary
    top_docs_text_per_topic[int(topic_id)] = top_docs_texts

# Now top_docs_text_per_topic contains the actual document texts for each topic
# You can print or process it further
for topic_id, docs in top_docs_text_per_topic.items():
    print(f"Topic {topic_id}:")
    for i, doc in enumerate(docs, 1):
        print(f"  Document {i}: {doc}\n")'''

import numpy as np
from collections import defaultdict

# Function to get the dominant topic for each document
def get_dominant_topic(lda_model, corpus):
    topics = []
    for bow in corpus:
        topic_distribution = lda_model.get_document_topics(bow)
        dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]
        topics.append(dominant_topic)
    return topics

# Get dominant topic for each document
document_topics = get_dominant_topic(lda_model, corpus)

# Initialize a dictionary to hold documents for each topic
topic_documents = defaultdict(list)

# Group documents by their dominant topic
for i, topic_id in enumerate(document_topics):
    topic_documents[topic_id].append(documents[i])

topic_documents

topic_documents.items()

# Print the grouped documents for each topic
for topic_id, docs in topic_documents.items():
    topic_words = " ".join([word for word, _ in lda_model.show_topic(topic_id, topn=150)])
    print(f"Topic {topic_id}: {topic_words}")
    print(f"Documents:")
    for doc in docs:
        print(f" - {doc}")

for doc in corpus:
    doc_topics = lda_model.get_document_topics(doc)
    dominant_topic = max(doc_topics, key=lambda x: x[1])
    print(f"Dominant Topic: {dominant_topic[0]} with probability {dominant_topic[1]}")
    filtered_df['topic'] = dominant_topic[0]
    filtered_df['topic_probability'] = dominant_topic[1]

filtered_df.head()

"""### Representaion Models"""

!pip install transformers torch gensim

token = "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"
from huggingface_hub import notebook_login
notebook_login()

!pip install transformers
!pip install keybert
!pip install sklearn

from transformers import pipeline
from keybert import KeyBERT
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np

# Initialize models
keybert_model = KeyBERT()

df= filtered_df.copy()

doc_topic_matrix

df.shape

df.describe()

df.info()

topic_words

"""## Bart Model"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Initialize BART model and tokenizer
mname = "facebook/bart-large-cnn"  # You can choose a different BART model if needed
tokenizer = AutoTokenizer.from_pretrained(mname)
model = AutoModelForSeq2SeqLM.from_pretrained(mname)

top_docs_per_topic= top_documents_per_topic

def generate_topic_summary(topic_texts):
    combined_text = " ".join(topic_texts)

    # Tokenize combined text
    inputs = tokenizer(combined_text, return_tensors="pt", max_length=512, truncation=True)

    # Generate summary
    summary_ids = model.generate(
        inputs.input_ids,
        max_new_tokens=20,  # Generate up to 50 new tokens
        min_length=3,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True,# Use attention mask for reliable results
    )

    # Decode and return summary
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

from collections import defaultdict

top_n = 5  # Number of top documents to consider per topic
top_docs_per_topic_lda = defaultdict(list)

for doc_id, bow in enumerate(corpus):
    # Get the topic distribution for the document
    topic_distribution = lda_model.get_document_topics(bow)

    # Sort topics by their weight for this document
    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)

    # Add the document ID to the list of top documents for the relevant topics
    for topic_id, score in sorted_topics:
        top_docs_per_topic_lda[topic_id].append((doc_id, score))

# Select top N documents per topic
for topic_id in top_docs_per_topic_lda:
    # Sort the documents by their score within each topic
    top_docs_per_topic_lda[topic_id] = sorted(top_docs_per_topic_lda[topic_id], key=lambda x: x[1], reverse=True)[:top_n]

# Now retrieve the texts for the top documents
top_docs_text_per_topic_lda = {}

for topic_id, doc_scores in top_docs_per_topic_lda.items():
    # Retrieve the actual documents using the identifiers
    top_docs_texts = [documents[doc_id] for doc_id, _ in doc_scores]

    # Store these documents in the dictionary
    top_docs_text_per_topic_lda[topic_id] = top_docs_texts

# Now top_docs_text_per_topic_lda contains the actual document texts for each topic
# You can print or process it further
for topic_id, docs in top_docs_text_per_topic_lda.items():
    print(f"Topic {topic_id}:")
    for i, doc in enumerate(docs, 1):
        print(f"  Document {i}: {doc}\n")

top_docs_per_topic= top_docs_per_topic_lda

summaries_per_topic_wise_bartlda = {}
for topic_id, top_docs in top_docs_per_topic.items():
    # Extract only the document texts using the document IDs
    topic_texts = [documents[doc_id] for doc_id, _ in top_docs]

    # Generate summary
    summary = generate_topic_summary(topic_texts)
    summaries_per_topic_wise_bartlda[topic_id] = summary

# Print the summaries
for topic_id, summary in summaries_per_topic_wise_bartlda.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")

summaries_per_topic_wise_bartlda

documents

"""## Keybert"""

documents= df['cleaned_text'].tolist()

from sklearn.feature_extraction.text import CountVectorizer
from keybert import KeyBERT

# Initialize KeyBERT model
keybert_model = KeyBERT()

# Function to get topic keywords using KeyBERT
def get_keybert_keywords(topic_words):
    # Combine the top words into a single string
    topic_text = ' '.join(topic_words)
    # Extract keywords using KeyBERT
    keywords = keybert_model.extract_keywords(topic_text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=5)
    return keywords

# Extract topic words from LDA model
num_top_words = 10  # Number of top words to consider per topic
topics = lda_model.get_topics()  # Get the topic distributions from the LDA model
feature_names = lda_model.id2word  # The vocabulary used by LDA model

# Initialize a list to hold keywords for each topic
topic_keywords_key_lda = []
lda_top_keyberta=[]
for topic_idx, topic in enumerate(topics):
    # Get top words for the topic
    top_words_indices = topic.argsort()[-num_top_words:]
    top_words = [feature_names[i] for i in top_words_indices]

    # Extract keywords using KeyBERT
    keywords = get_keybert_keywords(top_words)
    topic_keywords_key_lda.append(keywords)

# Print extracted keywords for each topic
for idx, keywords in enumerate(topic_keywords_key_lda):
    print(f"Topic {idx}: {[kw[0] for kw in keywords]}")
    lda_top_keyberta.append([kw[0] for kw in keywords])

lda_top_keyberta

!pip install transformers
import transformers
from transformers import GPT2LMHeadModel, GPT2Tokenizer

"""## GPT"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def generate_summary_gpt(text):
    # Prepare input for GPT
    input_ids = tokenizer.encode(f"Please provide a concise summary of the following text:\n{text}", return_tensors='pt')

    # Generate summary
    outputs = model.generate(
        input_ids,
        max_length=1000,  # Adjust based on desired summary length
        num_beams=4,
        no_repeat_ngram_size=2,
        early_stopping=True,
        pad_token_id=tokenizer.eos_token_id  # Handle padding
    )

    # Decode and return summary
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def combine_documents(docs):
    return " ".join(docs)

combined_texts = {topic_id: combine_documents([documents[doc_id] for doc_id, _ in top_docs])
                  for topic_id, top_docs in top_docs_per_topic.items()}

# Generate summaries or labels for each topic
'''
summaries_per_topic = {topic_id: generate_summary_gpt(text) for topic_id, text in combined_texts.items()}

# Print summaries
for topic_id, summary in summaries_per_topic.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")'''

gpt1 = [
   "Casual and Emotional Support",
 "Event Information and Seeking Help",
"Fan Content and Interaction",
 "Personal Updates and Experiences",
 "Daily Life and Random Thoughts"
]

df_result_lda= pd.DataFrame({'Keybert Keywords': lda_top_keyberta})
df_result_lda['GPT'] = gpt1
df_result_lda['BART']= summaries_per_topic_wise_bartlda

"""## Representation Models - Results for LDA"""

#for LDA(Latent Dirichlet allocation)
df_result_lda.head()

"""# NMF"""

from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary

documents = filtered_df['cleaned_text'].tolist()

vect = TfidfVectorizer(stop_words='english',max_features=500)

# Fit and transform the flattened texts
X = tfidf_matrix

documents= filtered_df['cleaned_text'].tolist()

N_TOPICS = 5
nmf = NMF(n_components=N_TOPICS,alpha_H=0.3)
#nmf = NMF(n_components=N_TOPICS)
W = nmf.fit_transform(X)  # Document-topic matrix
H = nmf.components_     # Topic-term matrix

# Create TF-IDF matrix
vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform(documents)
tfidf_feature_names = vectorizer.get_feature_names_out()
vect= vectorizer

tokenized_docs= [] # Initialize tokenized_docs as a list
for doc in documents:
    tokens = word_tokenize(doc)
    pos_tags = pos_tag(tokens)
    filtered_tokens = [word for word, pos in pos_tags if pos not in pos_to_remove]
    tokenized_docs.append(filtered_tokens) # Append to the list
    filtered_documents.append(' '.join(filtered_tokens))

no_top_words = 3
nmf_topic_words = []
for topic in H:
    topic_words = [tfidf_feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
    nmf_topic_words.append(topic_words)

X_reconstructed = np.dot(W, H)
X_array = X.toarray()  # Convert sparse matrix to dense array

# Max Norm (Infinity Nor
reconstruction_error_max = np.linalg.norm(X_array - X_reconstructed, np.inf)
print(f'Reconstruction Error: {reconstruction_error_max}')

"""### Coherene Score"""

from gensim.models.coherencemodel import CoherenceModel

# Function to calculate coherence scores
def calculate_coherence_scores(topics, texts, dictionary, coherence_types=['u_mass', 'c_v', 'c_uci', 'c_npmi']):
    coherence_scores = {}
    for coherence in coherence_types:
        coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence=coherence)
        coherence_scores[coherence] = coherence_model.get_coherence()
    return coherence_scores

# Calculate coherence scores for NMF
coherence_scores_nmf = calculate_coherence_scores(nmf_topic_words, tokenized_docs, dictionary)
print('NMF Coherence Scores:', coherence_scores_nmf)

W

H

W.shape

import numpy as np

words = np.array(vect.get_feature_names_out())
topic_words = pd.DataFrame(np.zeros((N_TOPICS, 5)), index=[f'Topic {i + 1}' for i in range(N_TOPICS)],
                           columns=[f'Word {i + 1}' for i in range(5)]).astype(str)

for i in range(N_TOPICS):
    ix = H[i].argsort()[::-1][:5]
    topic_words.iloc[i] = words[ix]

topic_words

words = np.array(vect.get_feature_names_out())
topic_words = pd.DataFrame(np.zeros((N_TOPICS, 10)), index=[f'Topic {i + 1}' for i in range(N_TOPICS)],
                           columns=[f'Word {i + 1}' for i in range(10)]).astype(str)

for i in range(N_TOPICS):
    ix = H[i].argsort()[::-1][:10]
    topic_words.iloc[i] = words[ix]

topic_words

import numpy as np

words = np.array(vect.get_feature_names_out())
topic_words = pd.DataFrame(np.zeros((N_TOPICS, 15)), index=[f'Topic {i + 1}' for i in range(N_TOPICS)],
                           columns=[f'Word {i + 1}' for i in range(15)]).astype(str)

for i in range(N_TOPICS):
    ix = H[i].argsort()[::-1][:15]
    topic_words.iloc[i] = words[ix]

topic_words

topic_mapping = {
    # Emotional, Appraisal, informational, instrumental support...
    'Topic 1' : 'Expressions of Emotional and Social Support',
    'Topic 2' : 'Celebrations and Congratulatory Messages',
    'Topic 3' :  'Requests for Help or Opinions',
    'Topic 4' : 'Greetings and Daily Updates',
    'Topic 5' : 'Reflections on Personal Relationships and Support'
}

import numpy as np
import pyLDAvis
import pyLDAvis.gensim_models
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from gensim.corpora.dictionary import Dictionary
from gensim.matutils import Sparse2Corpus

"""## pyLDAvis Visualization"""

# Convert topic-term matrix to probabilities
topic_term_dists = H / H.sum(axis=1)[:, None]

# Document-topic distributions
doc_topic_dists = W / W.sum(axis=1)[:, None]

# Document lengths
doc_lengths = [len(doc.split()) for doc in documents]

# Term frequency: sum up tf-idf scores across all documents
term_frequency = np.asarray(X.sum(axis=0)).flatten()

# Create vocabulary dictionary
vocab = vectorizer.get_feature_names_out()

# Create pyLDAvis data
prepared_data = {
    'topic_term_dists': topic_term_dists,
    'doc_topic_dists': doc_topic_dists,
    'doc_lengths': doc_lengths,
    'vocab': vocab,
    'term_frequency': term_frequency
}

# Generate the pyLDAvis visualization
vis_data = pyLDAvis.prepare(
    topic_term_dists=prepared_data['topic_term_dists'],
    doc_topic_dists=prepared_data['doc_topic_dists'],
    doc_lengths=prepared_data['doc_lengths'],
    vocab=prepared_data['vocab'],
    term_frequency=prepared_data['term_frequency']
)

pyLDAvis.display(vis_data)

# To save as an HTML file:

pyLDAvis.save_html(vis_data, 'nmf_pylDAvis.html')

W = pd.DataFrame(W, columns=[f'Topic {i + 1}' for i in range(N_TOPICS)])
W['max_topic'] = W.apply(lambda x: topic_mapping.get(x.idxmax()), axis=1)
W[pd.notnull(W['max_topic'])].head(10)

"""### Top Documents"""

def get_top_documents_for_topic(topic_id, W, documents, num_top_docs=5):
    # Access column by index for Pandas DataFrame using .iloc
    doc_weights = W.iloc[:, topic_id]  # Use .iloc for column access
    top_docs_indices = doc_weights.argsort()[::-1][:num_top_docs]
    top_docs = [(i, doc_weights.iloc[i], filtered_df['text'].iloc[i]) for i in top_docs_indices] # Use .iloc for integer-location based indexing for Pandas Series
    return top_docs

# Print top 5 documents for each
N_TOPICS=5
for topic_id in range(N_TOPICS):
    top_docs = get_top_documents_for_topic(topic_id, W, documents, num_top_docs=5)
    print(f"\nTop documents for Topic {topic_id + 1}:")
    for idx, weight, doc in top_docs:
        print(f"Document: {idx}, Weight: {weight:.4f}")
        print(f"  {doc}")
        print()

top_docs

import numpy as np
import pandas as pd

# Convert to DataFrame for easier manipulation
doc_topic_df = pd.DataFrame(W)

# Initialize dictionary to store top documents per topic
top_n = 5  # Number of top documents to consider per topic
top_docs_per_topic_nmf = {}

for topic_id in range(doc_topic_df.shape[1]):
    # Sort documents by their topic score for the current topic
    sorted_docs = doc_topic_df.iloc[:, topic_id].sort_values(ascending=False)

    # Get the indices of the top N documents
    top_docs = sorted_docs.head(top_n).index.tolist()

    # Store these indices in the dictionary
    top_docs_per_topic_nmf[topic_id] = top_docs

# Now top_docs_per_topic contains the indices of the top documents for each topic
# Next, retrieve the actual document texts
top_docs_text_per_topic_nmf = {}

for topic_id, doc_indices in top_docs_per_topic_nmf.items():
    # Retrieve the actual documents using the indices
    top_docs_texts = [documents[i] for i in doc_indices]

    # Store these documents in the dictionary
    top_docs_text_per_topic_nmf[int(topic_id)] = top_docs_texts

# Print or process the document texts further
for topic_id, docs in top_docs_text_per_topic_nmf.items():
    print(f"Topic {topic_id}:")
    for i, doc in enumerate(docs, 1):
        print(f"  Document {i}: {doc}\n")

top_docs_text_per_topic_nmf

"""### BART"""

summaries_per_topic_nmf_bart = {}

for topic_id, doc_indices in top_docs_per_topic_nmf.items():
    # Get the list of top document texts for the current topic
    top_docs = top_docs_text_per_topic_nmf[topic_id]

    # Generate a summary using the BART model
    summary = generate_topic_summary(top_docs)

    # Store the summary in the dictionary
    summaries_per_topic_nmf_bart[topic_id] = summary

# Print the summaries for each topic
for topic_id, summary in summaries_per_topic_nmf_bart.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")

combined_texts

# Now generate summaies for each topic
'''
summaries_per_topic_nmf_gpt = {topic_id: generate_summary_gpt(text) for topic_id, text in top_docs_text_per_topic_nmf.items()}

# Print the summaries
for topic_id, summary in summaries_per_topic_nmf_gpt.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")'''

num_top_words = 10  # Number of top words to consider per topic

# Assuming you have an NMF model and a vectorizer
topics = nmf.components_  # Get the topic distributions from the NMF model
feature_names = vectorizer.get_feature_names_out()  # The vocabulary used by the vectorizer

# Initialize a list to hold keywords for each topic
topic_keywords_key_nmf = []
nmf_top_keyberta = []

for topic_idx, topic in enumerate(topics):
    # Get top words for the topic
    top_words_indices = topic.argsort()[-num_top_words:]
    top_words = [feature_names[i] for i in top_words_indices]

    # Extract keywords using KeyBERT
    keywords = get_keybert_keywords(top_words)
    topic_keywords_key_nmf.append(keywords)

# Print extracted keywords for each topic
for idx, keywords in enumerate(topic_keywords_key_nmf):
    print(f"Topic {idx}: {[kw[0] for kw in keywords]}")
    nmf_top_keyberta.append([kw[0] for kw in keywords])

labels = [
    'Emotional Support and Personal Struggles',
    'Celebrations and Achievements',
    'Requests for Help',
    'Greetings and Daily Updates',
    'Missing and Nostalgia'
]

df_result_nmf= pd.DataFrame({'Keybert Keywords': nmf_top_keyberta})
df_result_nmf['GPT'] = labels
df_result_nmf['BART']= summaries_per_topic_nmf_bart

"""### Results for NMF"""

df_result_nmf.head()

for topic_id in range(N_TOPICS):
    top_docs = get_top_documents_for_topic(topic_id, W, documents, num_top_docs=10)
    print(f"\nTop documents for Topic {topic_id + 1}:")
    for idx, weight, doc in top_docs:
        print(f"Document: {idx}, Weight: {weight:.4f}")
        print(f"  {doc}")
        print()

"""# LSA"""

from wordcloud import WordCloud
#','.join(list(filtered_df['Text'].values))
# Join the different processed titles together.
long_string = ' '.join(documents) # Join the list of strings into a single string
# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=1000)

# Generate a word cloud
wordcloud.generate(long_string)

# Visualize the wordcloud
wordcloud.to_image()

from sklearn.decomposition import TruncatedSVD
lsa_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=10, random_state=42)
lsa_top=lsa_model.fit_transform(tfidf_matrix)

print(lsa_top)

lsa_top.shape
documents_series = pd.Series(documents)

l=lsa_top[0]
print("Documents :")
for i,topic in enumerate(l):
    print("Topic ",i," : ",topic*100)

import numpy as np
import pyLDAvis

# Normalize topic-term matrix
topic_term_dists = np.abs(lsa_model.components_) / np.sum(np.abs(lsa_model.components_), axis=1)[:, None]

# Normalize document-topic matrix
doc_topic_dists = lsa_top / np.sum(lsa_top, axis=1)[:, None]

# Create document lengths
doc_lengths = [len(doc.split()) for doc in documents]

# Term frequency
term_frequency = np.asarray(tfidf_matrix.sum(axis=0)).flatten()

# Create vocabulary list
vocab = vectorizer.get_feature_names_out()

# Prepare data for PyLDAVis
prepared_data = {
    'topic_term_dists': topic_term_dists,
    'doc_topic_dists': doc_topic_dists,
    'doc_lengths': doc_lengths,
    'vocab': vocab,
    'term_frequency': term_frequency
}

# Generate the PyLDAVis visualization
vis_data = pyLDAvis.prepare(
    topic_term_dists=prepared_data['topic_term_dists'],
    doc_topic_dists=prepared_data['doc_topic_dists'],
    doc_lengths=prepared_data['doc_lengths'],
    vocab=prepared_data['vocab'],
    term_frequency=prepared_data['term_frequency']
)

# Display the visualization
pyLDAvis.display(vis_data)

pyLDAvis.save_html(vis_data, 'lsa_visualization.html')

print(lsa_model.components_.shape) # (no_of_topics*no_of_words)
print(lsa_model.components_)

"""### Top Words"""

vocab = vect.get_feature_names_out()
for i, comp in enumerate(lsa_model.components_):
    vocab_comp = zip(vocab, comp)
    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]
    print("Topic "+str(i)+": ")
    for t in sorted_words:
        print(t[0],end=" ")
    print("\n")

# Tokenize each document in the 'documents' list
tokenized_documents = [doc.split() for doc in documents]
dictionary = Dictionary(tokenized_documents)
corpus = [dictionary.doc2bow(text) for text in tokenized_documents]

tokenized_docs

"""## Singular value analysis"""

import matplotlib.pyplot as plt

# Extract singular values
singular_values = lsa_model.singular_values_

# Plot singular values
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(singular_values) + 1), singular_values, 'o-', markersize=8)
plt.title('Singular Values of LSA Topics')
plt.xlabel('Topic Number')
plt.ylabel('Singular Value')
plt.grid(True)
plt.show()

print("Singular Values:")
for idx, value in enumerate(singular_values, start=1):
    print(f"Topic {idx}: Singular Value = {value}")

"""### Coherence Score"""

lsa_model.fit(tfidf_matrix)

# Get topics
topics = []
for i, comp in enumerate(lsa_model.components_):
    terms_comp = zip(vect.get_feature_names_out(), comp)
    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:5]
    topics.append([t[0] for t in sorted_terms])

# Calculate coherence score
coherence_model_lda = CoherenceModel(topics=topics, texts=filtered_df['Text'], dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()

print(f'Coherence Score: {coherence_lda}')

"""### Top Docs"""

# Function to get the top N documents for each topic
def get_top_documents_for_topic(topic_id, lsa_top, documents, num_top_docs=20):
    doc_weights = lsa_top[:, topic_id]
    top_docs_indices = doc_weights.argsort()[::-1][:num_top_docs]
    # Ensure that indices are within the valid range of 'documents'
    top_docs_indices = [i for i in top_docs_indices if i < len(documents)]
    # Return a list of tuples (index, weight, document)
    return [(i, doc_weights[i], filtered_df['text'].iloc[i]) for i in top_docs_indices]

N_TOPICS= 5

for topic_id in range(N_TOPICS):
    top_docs = get_top_documents_for_topic(topic_id, lsa_top, documents, num_top_docs=10)
    print(f"\nTop documents for Topic {topic_id + 1}:")
    # Iterate over the list of tuples
    for idx, weight, doc in top_docs:
        print(f"Document: {idx}, Weight: {weight:.4f}")
        print(f"  {doc}")
        print()

for topic_id in range(N_TOPICS):
    top_docs = get_top_documents_for_topic(topic_id, lsa_top, documents, num_top_docs= 5)
    print(f"\nTop documents for Topic {topic_id + 1}:")
    # Iterate over the list of tuples
    for idx, weight, doc in top_docs:
        print(f"Document: {idx}, Weight: {weight:.4f}")
        print(f"  {doc}")
        print()

"""Docs

"""

top_n = 5

# Dictionary to hold top documents for each topic
top_docs_per_topic_lsa = {}

# Iterate over each topic
for topic_id in range(lsa_top.shape[1]):
    # Get the scores for the current topic
    topic_scores = lsa_top[:, topic_id]

    # Get indices of top documents for the current topic
    top_doc_indices = np.argsort(topic_scores)[::-1][:top_n]

    # Store the indices of top documents for the current topic
    top_docs_per_topic_lsa[topic_id] = top_doc_indices

"""Text Top"""

top_docs_text_per_topic_lsa = {}

for topic_id, doc_indices in top_docs_per_topic_lsa.items():
    # Retrieve texts for the top documents of the current topic
    top_docs_texts = [documents[i] for i in doc_indices]
    top_docs_text_per_topic_lsa[topic_id] = top_docs_texts

top_docs_text_per_topic_lsa

"""### BART"""

summaries_per_topic_lsa_bart = {}

for topic_id, doc_indices in top_docs_per_topic_lsa.items():
    # Get the list of top document texts for the current topic
    top_docs = top_docs_text_per_topic_lsa[topic_id]

    # Generate a summary using the BART model
    summary = generate_topic_summary(top_docs)

    # Store the summary in the dictionary
    summaries_per_topic_lsa_bart[topic_id] = summary

# Print the summaries for each topic
for topic_id, summary in summaries_per_topic_lsa_bart.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")

"""### GPT"""

# Generate summaries for each topic
'''
summaries_per_topic_lsa_gpt = {topic_id: generate_summary_gpt(texts) for topic_id, texts in top_docs_text_per_topic_lsa.items()}

# Print summaries
for topic_id, summary in summaries_per_topic_lsa_gpt.items():
    print(f"Topic {topic_id} Summary:\n{summary}\n")'''

gpt = [
    'Emotional Support and Comfort',
    'Celebratory Moments and Birthdays',
    'Help and Requests',
    'Morning Greetings and Positivity',
    'Nostalgia and Farewell'
]

num_top_words = 10
topics = lsa_model.components_  # Get the topic distributions from the LSA model
feature_names = vectorizer.get_feature_names_out()  # The vocabulary used by the vectorizer

# Initialize a list to hold keywords for each topic
topic_keywords_key_lsa = []
lsa_top_keyberta = []

for topic_idx, topic in enumerate(topics):
    # Get top words for the topic
    top_words_indices = np.argsort(topic)[-num_top_words:]
    top_words = [feature_names[i] for i in top_words_indices]

    # Extract keywords using KeyBERT
    keywords = get_keybert_keywords(top_words)
    topic_keywords_key_lsa.append(keywords)

# Print extracted keywords for each topic
for idx, keywords in enumerate(topic_keywords_key_lsa):
    print(f"Topic {idx}: {[kw[0] for kw in keywords]}")
    lsa_top_keyberta.append([kw[0] for kw in keywords])

df_result_lsa= pd.DataFrame({'Keybert Keywords': lsa_top_keyberta})
df_result_lsa['GPT'] = gpt
df_result_lsa['BART']= summaries_per_topic_lsa_bart

"""### Results for LSA"""

df_result_lsa.head()

df_result_nmf.head()

df_result_lda.head()

!pip freeze

!pip freeze > requirements.txt

from google.colab import files
files.download('requirements.txt')

